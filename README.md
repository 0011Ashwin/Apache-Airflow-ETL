# üå§Ô∏è Weather ETL Pipeline with Apache Airflow

[![Apache Airflow](https://img.shields.io/badge/Apache%20Airflow-2.x-017CEE.svg)](https://airflow.apache.org/)
[![Docker](https://img.shields.io/badge/Docker-Ready-2496ED.svg)](https://docker.com)
[![PostgreSQL](https://img.shields.io/badge/PostgreSQL-15+-336791.svg)](https://postgresql.org)
[![Astronomer](https://img.shields.io/badge/Astronomer-Runtime-purple.svg)](https://astronomer.io)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

> A production-ready **ETL (Extract, Transform, Load)** pipeline built with Apache Airflow that fetches real-time weather data from the Open-Meteo API and stores it in PostgreSQL for analytics and monitoring.

---

## üìã Table of Contents

- [Overview](#-overview)
- [Architecture](#-architecture)
- [Features](#-features)
- [Prerequisites](#-prerequisites)
- [Installation](#-installation)
- [Usage](#-usage)
- [Project Structure](#-project-structure)
- [DAG Details](#-dag-details)
- [Configuration](#-configuration)
- [Monitoring](#-monitoring)
- [Contributing](#-contributing)
- [License](#-license)

---

## üîç Overview

This project demonstrates a **real-world ETL pipeline** using Apache Airflow, showcasing:

- **Data Engineering Best Practices** - Modular, testable, and maintainable code
- **Modern Orchestration** - Apache Airflow for workflow management
- **Cloud-Ready Architecture** - Containerized with Docker, deployable to Astronomer
- **API Integration** - RESTful API consumption using Airflow hooks
- **Database Operations** - PostgreSQL integration with proper schema management

### Use Cases

- üå°Ô∏è Weather monitoring and historical analysis
- üìä Building datasets for ML weather prediction
- üîî Alert systems based on weather conditions
- üìà Business intelligence dashboards

---

## üèóÔ∏è Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                         Apache Airflow                                   ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
‚îÇ  ‚îÇ                    Weather ETL Pipeline DAG                      ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ                                                                  ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ   EXTRACT    ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  TRANSFORM   ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ     LOAD     ‚îÇ      ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ              ‚îÇ    ‚îÇ              ‚îÇ    ‚îÇ              ‚îÇ      ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ Open-Meteo   ‚îÇ    ‚îÇ Data Clean   ‚îÇ    ‚îÇ PostgreSQL   ‚îÇ      ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ API Call     ‚îÇ    ‚îÇ & Structure  ‚îÇ    ‚îÇ Insert       ‚îÇ      ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ                                                                  ‚îÇ    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
‚îÇ                                                                          ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îÇ
‚îÇ  ‚îÇ  Postgres  ‚îÇ  ‚îÇ Webserver  ‚îÇ  ‚îÇ Scheduler  ‚îÇ  ‚îÇ  Triggerer ‚îÇ        ‚îÇ
‚îÇ  ‚îÇ (Metadata) ‚îÇ  ‚îÇ   :8080    ‚îÇ  ‚îÇ            ‚îÇ  ‚îÇ            ‚îÇ        ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚îÇ
                              ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                        External Services                                 ‚îÇ
‚îÇ                                                                          ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê           ‚îÇ
‚îÇ  ‚îÇ   Open-Meteo API    ‚îÇ              ‚îÇ  PostgreSQL (Data)  ‚îÇ           ‚îÇ
‚îÇ  ‚îÇ                     ‚îÇ              ‚îÇ                     ‚îÇ           ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Weather forecast  ‚îÇ              ‚îÇ ‚Ä¢ weather_data      ‚îÇ           ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Current weather   ‚îÇ              ‚îÇ ‚Ä¢ Historical store  ‚îÇ           ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Historical data   ‚îÇ              ‚îÇ ‚Ä¢ Analytics ready   ‚îÇ           ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## ‚ú® Features

### ETL Pipeline Features

| Feature | Description |
|---------|-------------|
| **Daily Scheduling** | Automated daily weather data collection |
| **Idempotent Operations** | Safe to re-run without data duplication |
| **Error Handling** | Robust error handling with retries |
| **Modular Tasks** | Separate Extract, Transform, Load tasks |
| **TaskFlow API** | Modern Airflow 2.x TaskFlow decorators |

### Infrastructure Features

| Feature | Description |
|---------|-------------|
| **Docker Compose** | One-command local development setup |
| **Astronomer Runtime** | Production-ready Airflow image |
| **PostgreSQL** | Reliable data storage |
| **Web UI** | Full Airflow web interface for monitoring |

### Data Captured

| Field | Type | Description |
|-------|------|-------------|
| `latitude` | FLOAT | Location latitude |
| `longitude` | FLOAT | Location longitude |
| `temperature` | FLOAT | Current temperature (¬∞C) |
| `windspeed` | FLOAT | Wind speed (km/h) |
| `winddirection` | FLOAT | Wind direction (degrees) |
| `weathercode` | INT | WMO weather code |
| `timestamp` | TIMESTAMP | Data collection time |

---

## üìã Prerequisites

- **Docker** & **Docker Compose** (v2.0+)
- **Astronomer CLI** (optional, for deployment)
- **Python 3.8+** (for local development)

---

## üöÄ Installation

### Method 1: Using Astronomer CLI (Recommended)

```bash
# Install Astronomer CLI
curl -sSL install.astronomer.io | sudo bash -s

# Clone the repository
git clone https://github.com/0011Ashwin/Apache-Airflow-ETL.git
cd Apache-Airflow-ETL

# Start Airflow locally
astro dev start
```

### Method 2: Using Docker Compose

```bash
# Clone the repository
git clone https://github.com/0011Ashwin/Apache-Airflow-ETL.git
cd Apache-Airflow-ETL

# Start services
docker-compose up -d

# Verify containers are running
docker ps
```

### Access the Airflow UI

- **URL**: http://localhost:8080
- **Username**: `admin`
- **Password**: `admin`

---

## üíª Usage

### 1. Configure Connections

In Airflow UI, go to **Admin ‚Üí Connections** and add:

#### PostgreSQL Connection
| Field | Value |
|-------|-------|
| Connection Id | `postgres_default` |
| Connection Type | `Postgres` |
| Host | `postgres` |
| Schema | `postgres` |
| Login | `postgres` |
| Password | `postgres` |
| Port | `5432` |

#### HTTP Connection (Open-Meteo API)
| Field | Value |
|-------|-------|
| Connection Id | `open_meteo_api` |
| Connection Type | `HTTP` |
| Host | `https://api.open-meteo.com` |

### 2. Enable and Trigger DAG

1. Go to **DAGs** in the Airflow UI
2. Find `weather_etl_pipeline`
3. Toggle the DAG to **ON**
4. Click **Trigger DAG** to run manually

### 3. Monitor Execution

- View task logs in **Graph View**
- Check **Task Instance Details** for debugging
- Monitor **Gantt Chart** for performance

### 4. Query the Data

```sql
-- Connect to PostgreSQL and run:
SELECT * FROM weather_data 
ORDER BY timestamp DESC 
LIMIT 10;

-- Get average temperature over time
SELECT 
    DATE(timestamp) as date,
    AVG(temperature) as avg_temp,
    AVG(windspeed) as avg_wind
FROM weather_data
GROUP BY DATE(timestamp)
ORDER BY date DESC;
```

---

## üìÅ Project Structure

```
Apache-Airflow-ETL/
‚îú‚îÄ‚îÄ dags/                           # Airflow DAG definitions
‚îÇ   ‚îú‚îÄ‚îÄ etlweather.py               # Main Weather ETL pipeline
‚îÇ   ‚îú‚îÄ‚îÄ exampledag.py               # Example DAG template
‚îÇ   ‚îî‚îÄ‚îÄ .airflowignore              # Files to ignore
‚îÇ
‚îú‚îÄ‚îÄ tests/                          # Unit tests for DAGs
‚îÇ
‚îú‚îÄ‚îÄ Dockerfile                      # Astronomer Runtime image
‚îú‚îÄ‚îÄ docker-compose.yml              # Local development setup
‚îú‚îÄ‚îÄ packages.txt                    # OS-level dependencies
‚îú‚îÄ‚îÄ requirements.txt                # Python dependencies
‚îî‚îÄ‚îÄ README.md                       # Documentation
```

---

## üìä DAG Details

### weather_etl_pipeline

```python
DAG Configuration:
‚îú‚îÄ‚îÄ dag_id: weather_etl_pipeline
‚îú‚îÄ‚îÄ schedule_interval: @daily
‚îú‚îÄ‚îÄ catchup: False
‚îî‚îÄ‚îÄ default_args:
    ‚îú‚îÄ‚îÄ owner: airflow
    ‚îî‚îÄ‚îÄ start_date: days_ago(1)

Tasks:
‚îú‚îÄ‚îÄ extract_weather_data()    # Fetch from Open-Meteo API
‚îú‚îÄ‚îÄ transform_weather_data()  # Clean and structure data
‚îî‚îÄ‚îÄ load_weather_data()       # Insert into PostgreSQL
```

### Task Dependencies

```
extract_weather_data ‚Üí transform_weather_data ‚Üí load_weather_data
```

### Customization

Modify these variables in `etlweather.py` to change location:

```python
LATITUDE = '51.5074'    # London
LONGITUDE = '-0.1278'
```

---

## ‚öôÔ∏è Configuration

### Environment Variables

| Variable | Default | Description |
|----------|---------|-------------|
| `AIRFLOW__CORE__EXECUTOR` | `LocalExecutor` | Airflow executor type |
| `AIRFLOW__DATABASE__SQL_ALCHEMY_CONN` | - | Metadata DB connection |
| `POSTGRES_USER` | `postgres` | PostgreSQL username |
| `POSTGRES_PASSWORD` | `postgres` | PostgreSQL password |

### Scaling for Production

```yaml
# docker-compose.yml modifications for production
services:
  scheduler:
    deploy:
      replicas: 2
  worker:
    deploy:
      replicas: 4
```

---

## üìà Monitoring

### Airflow UI Dashboards

- **DAG Runs**: Track execution history
- **Task Duration**: Monitor performance trends
- **Gantt Chart**: Visualize task parallelism
- **Logs**: Debug task failures

### Metrics to Watch

| Metric | Alert Threshold | Description |
|--------|-----------------|-------------|
| DAG Duration | > 5 minutes | Pipeline taking too long |
| Task Failures | > 0 | Any task failure |
| API Response Time | > 10 seconds | Slow API calls |

---

## ü§ù Contributing

Contributions are welcome! Here's how to get started:

1. **Fork** the repository
2. **Create** a feature branch (`git checkout -b feature/NewDAG`)
3. **Add tests** for your changes
4. **Commit** your changes (`git commit -m 'Add multi-city weather DAG'`)
5. **Push** to the branch (`git push origin feature/NewDAG`)
6. **Open** a Pull Request

### Ideas for Contributions

- [ ] Add multi-location support
- [ ] Implement data quality checks
- [ ] Add weather alerts DAG
- [ ] Create visualization dashboard
- [ ] Add historical backfill DAG

---

## üìö Resources

- [Apache Airflow Documentation](https://airflow.apache.org/docs/)
- [Astronomer Learn](https://www.astronomer.io/docs/learn/)
- [Open-Meteo API Docs](https://open-meteo.com/en/docs)
- [TaskFlow API Tutorial](https://airflow.apache.org/docs/apache-airflow/stable/tutorial/taskflow.html)

---

## üìÑ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

<p align="center">
  Built with ‚ù§Ô∏è for data engineering
  <br>
  <a href="https://github.com/0011Ashwin">@0011Ashwin</a>
</p>
